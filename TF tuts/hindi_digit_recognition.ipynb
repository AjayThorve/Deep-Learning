{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN to train and classify Devanagari Handwritten Character Dataset digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "root = 'tmp/DevanagariHandwrittenCharacterDataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts each label index in y to vector with one_hot encoding\n",
    "    One-hot encoding converts categorical labels to binary values\n",
    "    \"\"\"\n",
    "    y_one_hot = np.zeros((num_classes))\n",
    "    y_one_hot[y] = 1\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(path):\n",
    "    final_path = root+path\n",
    "    final_data_images = []\n",
    "    final_data_labels = []\n",
    "    for directory, subdirectories, files in os.walk(final_path):\n",
    "        for dr in subdirectories:\n",
    "            if \"digit\" in str(dr):\n",
    "                label = int(dr.split('_')[1])\n",
    "                dr_path = final_path+\"/\"+dr\n",
    "                for d,s,f in os.walk(dr_path):\n",
    "                    for i in f:\n",
    "                        image_data = imread(dr_path+\"/\"+i).flatten()\n",
    "                        image_data = image_data.reshape(image_data.shape[0])\n",
    "                        final_data_images.append(image_data)\n",
    "                        final_data_labels.append(one_hot(label))\n",
    "    return np.array(final_data_images),np.array(final_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data():\n",
    "    train_data,train_labels = getDataset(\"/train/\")\n",
    "    return train_data,train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data():\n",
    "    test_data,test_labels = getDataset(\"/test/\")\n",
    "    return test_data,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,train_labels = get_train_data()\n",
    "test_data,test_labels = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_train_batch(batch_size, current_index):\n",
    "    if(current_index >= len(train_data)):\n",
    "        current_index = 0\n",
    "    X_batch,y_batch = train_data[current_index:(current_index+batch_size)], train_labels[current_index:(current_index+batch_size)]\n",
    "    return X_batch, y_batch, (current_index+batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that the data preparation is done, lets train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of nodes in hidden layer 1, hl2, and hl3\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "# number of output classes = [0,1,2,3,4,5,6,7,8,9]\n",
    "n_classes = 10\n",
    "\n",
    "#due to RAM limit, its best to use batches for input processing\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder('float',[None,1024])\n",
    "y = tf.placeholder('float',[None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([1024,n_nodes_hl1])),\n",
    "                     'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "    \n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1,n_nodes_hl2])),\n",
    "                     'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "    \n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2,n_nodes_hl3])),\n",
    "                     'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "    \n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3,n_classes])),\n",
    "                     'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    # z = (input_data*weights) + biases\n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']),hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "    \n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']),hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    \n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']),hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    \n",
    "    output = tf.add(tf.matmul(l3,output_layer['weights']),output_layer['biases'])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=y))\n",
    "    #default learning rate = 0.001\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    n_epochs = 50\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "            current_index = 0\n",
    "            for _ in range(int(num_examples/batch_size)):\n",
    "                epoch_x,epoch_y,current_index = get_next_train_batch(batch_size, current_index)\n",
    "                _,c = sess.run([optimizer,cost], feed_dict = {x:epoch_x,y:epoch_y})\n",
    "                epoch_loss += c\n",
    "                \n",
    "            print('epoch', epoch, 'completed out of ', n_epochs, 'loss',epoch_loss)\n",
    "            if(epoch_loss == 0.0):\n",
    "                break\n",
    "        \n",
    "        correct = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct,'float'))\n",
    "        print('Accuracy', accuracy.eval({x:test_data,y:test_labels}))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 completed out of  50 loss 2586271742.732422\n",
      "epoch 1 completed out of  50 loss 537688947.125\n",
      "epoch 2 completed out of  50 loss 314669534.5566406\n",
      "epoch 3 completed out of  50 loss 194708601.13232422\n",
      "epoch 4 completed out of  50 loss 130557671.64398193\n",
      "epoch 5 completed out of  50 loss 97107493.21679688\n",
      "epoch 6 completed out of  50 loss 74342444.4621582\n",
      "epoch 7 completed out of  50 loss 58310928.73693848\n",
      "epoch 8 completed out of  50 loss 43121814.39984131\n",
      "epoch 9 completed out of  50 loss 29151392.747680664\n",
      "epoch 10 completed out of  50 loss 18992226.0732193\n",
      "epoch 11 completed out of  50 loss 12205553.91241455\n",
      "epoch 12 completed out of  50 loss 7739739.420471191\n",
      "epoch 13 completed out of  50 loss 4194705.902893066\n",
      "epoch 14 completed out of  50 loss 2168271.201789856\n",
      "epoch 15 completed out of  50 loss 866038.9546356201\n",
      "epoch 16 completed out of  50 loss 401898.6431121826\n",
      "epoch 17 completed out of  50 loss 214864.42293071747\n",
      "epoch 18 completed out of  50 loss 94488.9820022583\n",
      "epoch 19 completed out of  50 loss 26279.787384033203\n",
      "epoch 20 completed out of  50 loss 1420.449951171875\n",
      "epoch 21 completed out of  50 loss 0.0\n",
      "Accuracy 0.9406667\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
